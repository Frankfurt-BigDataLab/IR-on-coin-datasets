{"cells":[{"cell_type":"code","execution_count":4,"metadata":{"id":"Yim5Xei76nbD","cellView":"form","executionInfo":{"status":"ok","timestamp":1659448306409,"user_tz":-120,"elapsed":4247,"user":{"displayName":"Seb G","userId":"13562849965321848594"}}},"outputs":[],"source":["%%capture\n","#tensorflow_version 1.4.0\n","#!pip install h5py==2.10.0 --force-reinstall\n","\n","#@title ## D4N4 Image Recognition Notebook\n","#@markdown This Colab Notebook is a demonstration of our Image Recognition approach for the Corpus Nummorum dataset based on a VGG16 model.\n","#@markdown You can upload your own coin images for a prediction of our model or use several sample images provided by us.\n","#@markdown The first step is to open the \"Runtime\" dropdown menu above and then click \"Execute all\". Alternatively, you can press \"Ctrl+F9\" on your keyboard\n","#@markdown or you can click the small arrows in a white circle on the left side of every cell. The execution of all cells can take some \n","#@markdown time due to the availability of Colab resources and the download of our models to the temporary Colab storage.\n","#@markdown A small green check mark on the left shows the sucessful execution of a cell. You can take a look at the program code by clicking \n","#@markdown the \"Show Code\" button under this text. Double-clicking on this text closes the code cell again.\n","\n","from IPython.display import clear_output \n","import ipywidgets as widgets\n","from IPython.display import display\n","clear_output()\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","# Enables tensorflow operations without running a graph\n","#tf.enable_eager_execution()\n","\n","import os\n","import numpy as np\n","\n","from IPython.display import Image, display\n","import matplotlib.pyplot as plt\n","import matplotlib.cm as cm\n","import matplotlib.image as mpim\n","import matplotlib.gridspec as gridspec\n","import ast\n","import math\n","import skimage.transform\n","\n","from PIL import Image\n","import requests\n","from io import BytesIO\n","from bs4 import BeautifulSoup\n","import cv2\n","\n","if not os.path.exists(\"vgg16-2/\"):\n","  os.mkdir(\"vgg16-2\")\n","  !wget -O vgg16_types 'https://docs.google.com/uc?export=download&id=1Q0zGHZUA5LxKOqy91F2SFFej44juqb4A&confirm=t'\n","  !wget -O vgg16_mints 'https://docs.google.com/uc?export=download&id=1G_7wz3QmRCygGfxpoqBNAuX1gS2mZbQU&confirm=t'\n","  !wget -O dict_types.txt 'https://docs.google.com/uc?export=download&id=1qzs_b-99_4COkmIYXXBLqOft1JXTcicj&confirm=t'\n","  !wget -O dict_mints.txt 'https://docs.google.com/uc?export=download&id=1ERVh9hV80rBQZu1Xg9B_HQVP7GIbgz11&confirm=t'\n","\n","\n","if not os.path.exists(\"temp_storage/\"):\n","  os.mkdir(\"temp_storage\")\n","  os.mkdir(\"temp_storage/obv\")\n","  os.mkdir(\"temp_storage/rev\")\n","\n","obv_storage = \"temp_storage/obv/\"\n","rev_storage = \"temp_storage/rev/\"\n","last_conv_layer_name = \"block5_conv3\"\n","\n","\n","\n","model1 = tf.keras.models.load_model('vgg16_types/')\n","model2 = tf.keras.models.load_model('vgg16_mints')\n","\n","with open('dict_types.txt') as f:\n","    types = f.read()\n","    dict_types = d = ast.literal_eval(types)\n","\n","with open('dict_mints.txt') as f:\n","    mints = f.read()\n","    dict_mints = d = ast.literal_eval(mints)\n","\n","\n","def pull_images_by_id(type_id):\n","    src = \"https://www.corpus-nummorum.eu/types/\" + str(type_id)\n","    \n","    # search images\n","\n","    result = requests.get(src)\n","\n","    # check if url call was successful\n","    if result.status_code == 404:\n","        print(\"Type not yet published\")\n","        return\n","    else:\n","        if result.status_code == 200:\n","            soup =  BeautifulSoup(result.content, \"html.parser\")\n","        else:\n","            return \"Connection Failed\"\n","\n","    # search links\n","        try:\n","            thumb_0 = soup.find('div',{\"id\":\"thumb-0\"}).find('img',{\"id\":\"{coin-thumbnail-0\"})[\"src\"]\n","            thumb_1 = soup.find('div',{\"id\":\"thumb-1\"}).find('img',{\"id\":\"{coin-thumbnail-1\"})[\"src\"]\n","\n","            # pull images\n","            response_0 = requests.get(thumb_0)\n","            response_1 = requests.get(thumb_1)\n","\n","            img_0 = Image.open(BytesIO(response_0.content))\n","            img_1 = Image.open(BytesIO(response_1.content))\n","\n","            return img_0, img_1\n","        except:\n","            return \"Error\"\n","\n","def get_img_array(img_path, size):\n","    # `img` is a PIL image of size 299x299\n","    img = keras.preprocessing.image.load_img(img_path, target_size=size)\n","    # `array` is a float32 Numpy array of shape (224, 224, 3)\n","    array = keras.preprocessing.image.img_to_array(img)\n","    # We add a dimension to transform our array into a \"batch\"\n","    # of size (1, 224, 224, 3)\n","    array = np.expand_dims(array, axis=0)\n","    return array\n","\n","def top_5(preds):\n","    # Find the top-5 predictions\n","    top_5 = np.argsort(preds) # sorts indices according to their probability\n","    top_5 = top_5[0]\n","    top_5 = top_5[::-1] # we want the top-5 probabilities at the first 5 entries\n","    top_5 = top_5[0:5]\n","    probs = []\n","    top_5_new = []\n","    for item in top_5:\n","      if preds[0][item] > 0:\n","        probs.append(preds[0][item])\n","        top_5_new.append(item)\n","    return top_5_new, probs\n","\n","def translate(top5, classes):\n","    # Translates the numerical representation of classes to their respective names\n","    outcome = []\n","    if classes == \"types\":\n","      for key in top5:\n","        outcome.append(dict_types[key])\n","    if classes == \"mints\":\n","      for key in top5:\n","        outcome.append(dict_mints[key])\n","    return outcome\n","\n","def make_gradcam_heatmap(img_array, model, last_conv_layer_name, pred_index=None):\n","    # Makes the GradCam Heatmap (new version)\n","    grad_model = tf.keras.models.Model(\n","        [model.inputs], [model.get_layer(last_conv_layer_name).output, model.output]\n","    )\n","\n","    with tf.GradientTape() as tape:\n","        last_conv_layer_output, preds = grad_model(img_array)\n","        #print(\"hier\")\n","        #print(last_conv_layer_output)\n","        if pred_index is None:\n","            pred_index = tf.argmax(preds[0])\n","        class_channel = preds[:, pred_index]\n","\n","    grads = tape.gradient(class_channel, last_conv_layer_output)\n","    #print(grads)\n","    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n","\n","    last_conv_layer_output = last_conv_layer_output[0]\n","    heatmap = last_conv_layer_output @ pooled_grads[..., tf.newaxis]\n","    heatmap = tf.squeeze(heatmap)\n","    #print(heatmap)\n","    heatmap = tf.maximum(heatmap, 0) / tf.math.reduce_max(heatmap)\n","    return heatmap.numpy()\n","\n","def make_gradcam_heat(img_array, model, last_conv_layer_name, classifier_layer_names):\n","     # Makes the GradCam Heatmap (old version)\n","    # First, we create a model that maps the input image to the activations\n","    # of the last conv layer\n","    last_conv_layer = model.get_layer('vgg16').get_layer(last_conv_layer_name)\n","    last_conv_layer_model = keras.Model(model.get_layer('vgg16').inputs, last_conv_layer.output)\n","\n","    # Second, we create a model that maps the activations of the last conv\n","    # layer to the final class predictions\n","    classifier_input = keras.Input(shape=last_conv_layer.output.shape[1:])\n","    x = classifier_input\n","    for layer_name in classifier_layer_names:\n","        if layer_name == \"block5_pool\":\n","            x = model.get_layer('vgg16').get_layer(layer_name)(x)\n","        else:\n","            x = model.get_layer(layer_name)(x)\n","    classifier_model = keras.Model(classifier_input, x)\n","\n","    # Then, we compute the gradient of the top predicted class for our input image\n","    # with respect to the activations of the last conv layer\n","    with tf.GradientTape() as tape:\n","        # Compute activations of the last conv layer and make the tape watch it\n","        last_conv_layer_output = last_conv_layer_model(img_array)\n","        tape.watch(last_conv_layer_output)\n","        # Compute class predictions\n","        preds = classifier_model(last_conv_layer_output)\n","        top_pred_index = tf.argmax(preds[0])\n","        top_class_channel = preds[:, top_pred_index]\n","\n","    # This is the gradient of the top predicted class with regard to\n","    # the output feature map of the last conv layer\n","    grads = tape.gradient(top_class_channel, last_conv_layer_output)\n","    #print(tf.__version__)\n","    # This is a vector where each entry is the mean intensity of the gradient\n","    # over a specific feature map channel\n","    pooled_grads = tf.reduce_mean(grads, axis=(0, 1, 2))\n","\n","    # We multiply each channel in the feature map array\n","    # by \"how important this channel is\" with regard to the top predicted class\n","    last_conv_layer_output = last_conv_layer_output.numpy()[0]\n","    pooled_grads = pooled_grads.numpy()\n","    for i in range(pooled_grads.shape[-1]):\n","        last_conv_layer_output[:, :, i] *= pooled_grads[i]\n","\n","    # The channel-wise mean of the resulting feature map\n","    # is our heatmap of class activation\n","    heatmap = np.mean(last_conv_layer_output, axis=-1)\n","\n","    # For visualization purpose, we will also normalize the heatmap between 0 & 1\n","    heatmap = np.maximum(heatmap, 0) / np.max(heatmap)\n","    return heatmap\n","\n","def combine_image_heatmap(img_path, heatmap):\n","  # Combines the created heatmap and the source image\n","  try:\n","    img2 = keras.preprocessing.image.load_img(img_path)\n","    img2 = keras.preprocessing.image.img_to_array(img2)\n","    #img2 = np.fliplr(img2)\n","    # We rescale heatmap to a range 0-255\n","    heatmap = np.uint8(255 * heatmap)\n","\n","    # We use jet colormap to colorize heatmap\n","    jet = cm.get_cmap(\"jet\")\n","\n","    # We use RGB values of the colormap\n","    jet_colors = jet(np.arange(256))[:, :3]\n","    jet_heatmap = jet_colors[heatmap]\n","\n","    # We create an image with RGB colorized heatmap\n","    jet_heatmap = keras.preprocessing.image.array_to_img(jet_heatmap)\n","    jet_heatmap = jet_heatmap.resize((img2.shape[1], img2.shape[0]))\n","    jet_heatmap = keras.preprocessing.image.img_to_array(jet_heatmap)\n","\n","    # Superimpose the heatmap on original image\n","    superimposed_img = jet_heatmap * 0.3 + img2\n","    superimposed_img = keras.preprocessing.image.array_to_img(superimposed_img)\n","  except Exception as e:\n","    print(e)\n","  return superimposed_img\n","\n","# define a function for horizontally concatenating images of different heights \n","# Source: https://www.geeksforgeeks.org/concatenate-images-using-opencv-in-python/\n","def hconcat_resize(img_list, \n","                   interpolation \n","                   = cv2.INTER_CUBIC):\n","      # take minimum hights\n","    h_min = min(img.shape[0] \n","                for img in img_list)\n","      \n","    # image resizing \n","    im_list_resize = [cv2.resize(img,\n","                       (int(img.shape[1] * h_min / img.shape[0]),\n","                        h_min), interpolation\n","                                 = interpolation) \n","                      for img in img_list]\n","      \n","    # return final image\n","    return cv2.hconcat(im_list_resize)\n","\n","# resize the images to a square with padding\n","# Source: https://gist.github.com/CMCDragonkai/cf25ad969e3a7fc1e48d60ecf20a2253\n","\n","# you use this just before passing any image to a CNN\n","# which usually expects square images\n","# however your input images can be of variable size\n","# you don't want to just squash the images to a square\n","# because you will lose valuable aspect ratio information\n","# you want to resize while preserving the aspect ratio\n","# these 2 functions perform this resizing behaviour\n","# images are assumed to be formatted as Height, Width, Channels\n","# we will use bound_image_dim to first bound the image to a range\n","# the smallest dimension will be scaled up to the min_size\n","# the largest dimension will be scaled down to the max_size\n","# then afterwards you square_pad_image to pad the image to a square\n","# filling the empty space with zeros\n","\n","def bound_image_dim(image, min_size=None, max_size=None):\n","    if (max_size is not None) and \\\n","       (min_size is not None) and \\\n","       (max_size < min_size):\n","        raise ValueError('`max_size` must be >= to `min_size`')\n","    dtype = image.dtype\n","    (height, width, *_) = image.shape\n","    # scale the same for both height and width for fixed aspect ratio resize\n","    scale = 1\n","    # bound the smallest dimension to the min_size\n","    if min_size is not None:\n","        image_min = min(height, width)\n","        scale = max(1, min_size / image_min)\n","    # next, bound the largest dimension to the max_size\n","    # this must be done after bounding to the min_size\n","    if max_size is not None:\n","        image_max = max(height, width)\n","        if round(image_max * scale) > max_size:\n","            scale = max_size / image_max\n","    if scale != 1:\n","        image = skimage.transform.resize(\n","            image, (round(height * scale), round(width * scale)),\n","            order=1,\n","            mode='constant',\n","            preserve_range=True)\n","    return image.astype(dtype), image_max\n","\n","\n","def square_pad_image(image, size):\n","    (height, width, *_) = image.shape\n","    if (size < height) or (size < width):\n","        raise ValueError('`size` must be >= to image height and image width')\n","    pad_height = (size - height) / 2\n","    pad_top = math.floor(pad_height)\n","    pad_bot = math.ceil(pad_height)\n","    pad_width = (size - width) / 2\n","    pad_left = math.floor(pad_width)\n","    pad_right = math.ceil(pad_width)\n","    return np.pad(\n","        image, ((pad_top, pad_bot), (pad_left, pad_right), (0, 0)),\n","        mode='constant')\n","\n","def plot_types(type_list, probs, img, source_obv, source_rev):\n","    # Plot all images (source and predictes types)\n","    source = mpim.imread(img)\n","    fig = plt.figure(figsize=(6, 3.75))\n","    plt.imshow(source)\n","    plt.axis('off')\n","    plt.title(\"Combined source image\")\n","    plt.show()\n","    i = 0\n","    source_obv = mpim.imread(source_obv)\n","    source_rev = mpim.imread(source_rev)\n","    \n","    for item in type_list:\n","      i += 1\n","      print(str(i) + \". TypeId: \" + str(item)  + \", Probability: \" + str(probs[i-1]))\n","      print(\"Link to type: https://www.corpus-nummorum.eu/types/\"+ item)\n","      try:\n","        obv, rev = pull_images_by_id(item)\n","        rows = 1\n","\n","        if i < 8:\n","          fig = plt.figure(figsize=(6, 3.75), constrained_layout=True)\n","          spec = gridspec.GridSpec(ncols=4, nrows=1, figure=fig)\n","          columns = 4\n","          #source_obv = mpim.imread(source_obv)\n","          #source_rev = mpim.imread(source_rev)\n","          # showing obverse image         \n","          fig.add_subplot(spec[0, 0])\n","          plt.imshow(obv)\n","          plt.axis('off')\n","          plt.title(\"Obverse\")\n","          \n","          # showing reverse image\n","          fig.add_subplot(spec[0, 1])\n","          plt.imshow(rev)\n","          plt.axis('off')\n","          plt.title(\"Reverse\")\n","\n","          # showing source image\n","          fig.add_subplot(spec[0, 2])\n","          plt.imshow(source_obv)\n","          plt.axis('off')\n","          plt.title(\"Source Obverse\")\n","\n","          fig.add_subplot(spec[0, 3])\n","          plt.imshow(source_rev)\n","          plt.axis('off')\n","          plt.title(\"Source Reverse\")  \n","          \n","          plt.show()\n","        \n","        else:\n","          columns = 2\n","          fig = plt.figure(figsize=(4, 2.75))\n","          # showing obverse image \n","          fig.add_subplot(rows, columns, 1)\n","          plt.imshow(obv)\n","          plt.axis('off')\n","          plt.title(\"Obverse\")\n","          \n","          # showing reverse image\n","          fig.add_subplot(rows, columns, 2)\n","          plt.imshow(rev)\n","          plt.axis('off')\n","          plt.title(\"Reverse\")\n","          plt.show()\n","\n","        plt.show()\n","      except Exception as e:\n","        print()\n","def combine_images(img_obv, img_rev):\n","    # Concatenate obverse and reverse images and convert it to quadratic format\n","    # with horizontal paddings\n","    img_obverse = cv2.imread(img_obv)\n","    img_reverse = cv2.imread(img_rev)\n","    img_list = [img_obverse, img_reverse]\n","    if img_obverse.shape[0] != img_reverse.shape[0]:\n","        img_type = hconcat_resize(img_list)\n","\n","    else:\n","        img_type = cv2.hconcat(img_list)\n","    (height, width, *_) = img_type.shape\n","    max_size = max(height, width)\n","    min_size = min(height, width)              \n","    new_img, max_size = bound_image_dim(img_type, min_size, max_size)\n","    new_img = square_pad_image(new_img, max_size)\n","    cv2.imwrite(\"combined_image.jpg\", new_img)\n","    return \"combined_image.jpg\"\n","\n","def predict_type(img_path_obv, img_path_rev, model=model1):\n","  # Predicts an image with the type model\n","  try:\n","    img_path = combine_images(img_path_obv, img_path_rev)\n","    img_size = (224, 224)\n","    preprocess_input = keras.applications.resnet.preprocess_input\n","    img_array = preprocess_input(get_img_array(img_path, size=img_size))\n","    preds = model.predict(img_array)\n","    out, prob = top_5(preds)\n","    out = translate(out, \"types\")\n","    plot_types(out, prob, img_path, img_path_obv, img_path_rev)\n","\n","    #heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name) #, classifier_layer_names)\n","    #final_image = combine_image_heatmap(img_path, heatmap)\n","    #print(heatmap)\n","    #plt.imshow(final_image)\n","\n","    print()\n","  except Exception as e:\n","    print(e)\n","\n","\n","def predict_mint(img_path_obv, img_path_rev, model=model2):\n","   # Predicts an image with the mint model\n","  try:\n","    img_path = combine_images(img_path_obv, img_path_rev)\n","    img_size = (224, 224)\n","    preprocess_input = keras.applications.resnet.preprocess_input\n","    img_array = preprocess_input(get_img_array(img_path, size=img_size))\n","    preds = model.predict(img_array)\n","    out, prob = top_5(preds)\n","    out = translate(out, \"mints\")\n","    i = 0\n","    source = mpim.imread(img_path)\n","    fig = plt.figure(figsize=(5, 3.25))\n","    plt.title(\"Combined source image\")\n","    plt.imshow(source)\n","    plt.axis('off')\n","    plt.show()\n","    for item in out:\n","      i += 1\n","      new_item = item.replace(' ','+')\n","      print(str(i) + \". Mint: \" + str(item)  + \", Probability: \" + str(prob[i-1]))\n","      print(\"Link to mint: https://www.corpus-nummorum.eu/search/types?type=quicksearch&q=\"+ new_item)\n","    #print(out)\n","    #print(\"Most likely result: https://www.corpus-nummorum.eu/search/types?type=quicksearch&q=\" +out[0])\n","    #heatmap = make_gradcam_heatmap(img_array, model, last_conv_layer_name) #, classifier_layer_names)\n","    #final_image = combine_image_heatmap(img_path, heatmap)\n","    #gradcam, preds = plot_cam(filename)\n","    #plt.imshow(final_image)\n","    #plt.xlabel(str(preds[0])+\"\\n\"+str(preds[1])+\"\\n\"+str(preds[2]))\n","    #plt.xticks([])\n","    #plt.yticks([])\n","    print()\n","  except Exception as e:\n","    print(e)\n","\n"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"Jp67jHUDqjb9","colab":{"base_uri":"https://localhost:8080/","height":241,"referenced_widgets":["1f38db7575b44f349ee1fde7f1559227","2a2795f71f644e15998da5ab4a98a241","1ed8bc48bc1641cf8c209719ff3fddbf","2cf69a3bbb1142fe91a1397aec0f0fa4","b3dfcd3dd1d14a07a3a79d44a1765716","694db753a0924048be9f9371071c350c","580f9dc8e5554fe5af329c4940d9268b","8aed0295d0b245c7aeb72345db4c5e75","03468fcfe183407085a7de2dc5f12a32","c07e049111184fb4a4514d4d60cad3e1","58fb6de8a69a4842b4b178abb9deadae","4566812b86bf41cbb3f7a9807d8dca68","ca6c927cdec34004aaf3a3f097f015a8","82d4c88f3eee4f6d9976c25da1e35653","bc3ac467b13d47a88e43677b96b3fc2b","220540c641d247eca2a0e8b97a33bff6","959f88fee8734f72a4ab65fcc95bd9c1","3b8ee371af324de1bf4850432e5a5b93","07903b379bd945af8d0023d7b6bc17c7","5141989e342645bda0c8cd23c547d3a6","828c51f47b3f4009a2ad9cf2d621e78b","0ead38a6197b415db9f33b0cbc00de04","5eb2ce08e756467ca1a2cf2ae1008148","13767043fb7f4405b45bd29af9bb55df","8d175bd7abc947c9a62ee7640b12c9af","dcae0b73241b4154a0853a0737b463ec","b42eb490a3414f0e9a904eb43202e3e7","fe5e8c778a51489392d628cb9ae92660","5046131e4c7048e7a9facc8503d05f5e","f0e9eb26075a43518a3d27fd09ebd09e","07ab33424f8b4343b3974ed876d5d2a4","ecb1f37055614b86ac5ea85145fb3af7","dd12ba33be9046b484f250f26b54a5f0"]},"executionInfo":{"status":"ok","timestamp":1659448424978,"user_tz":-120,"elapsed":486,"user":{"displayName":"Seb G","userId":"13562849965321848594"}},"outputId":"0513ecd0-35a1-45e9-86bc-d92d53db3f90","cellView":"form"},"outputs":[{"output_type":"display_data","data":{"text/plain":["HBox(children=(FileUpload(value={}, description='Select Obverse', multiple=True), FileUpload(value={}, descripâ€¦"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f38db7575b44f349ee1fde7f1559227"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Button(description='Upload', style=ButtonStyle())"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03468fcfe183407085a7de2dc5f12a32"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Output()"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4566812b86bf41cbb3f7a9807d8dca68"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Button(description='Delete uploaded files', style=ButtonStyle())"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"82d4c88f3eee4f6d9976c25da1e35653"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Button(description='Delete Session', style=ButtonStyle())"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"959f88fee8734f72a4ab65fcc95bd9c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["HBox(children=(Dropdown(options=(), value=None), Dropdown(options=(), value=None)))"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5141989e342645bda0c8cd23c547d3a6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Button(description='Predict Type', style=ButtonStyle())"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fe5e8c778a51489392d628cb9ae92660"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Button(description='Predict Mint', style=ButtonStyle())"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07ab33424f8b4343b3974ed876d5d2a4"}},"metadata":{}}],"source":["#@title ## How to use the Image Recognition\n","#@markdown After both code cells were executed, you can use the model to predict coin types and mints on your images.\n","#@markdown For this you can upload your own coin images. Since the model is only trained for CN coins, it is recommended to use only coin photos of the ancient landscapes treated there.\n","#@markdown You can get coin images from www.corpus-nummorum.eu. Alternatively, we provide several coin photos from our test set here.\n","\n","#@markdown If you want to use your own images, you can choose obverse and reverse of a coin separate with the \"Select Obverse/Reverse\" button from your local drive. Afterwards, you have to click the \"Upload\" Button.\n","#@markdown You can also delete the uploaded files and the whole session with the respective button. The dropdown menus below contain the uploaded images. You can combine the matching images, if you \n","#@markdown have uploaded several coins. For a prediction of a coin you must click the \"Predict Type/Mint\" buttons. \"Predict Type\" will show you up to \n","#@markdown five types which were predicted by our model. \n","\n","#@markdown The first image shows you how your uploaded images were processed by our approach (called \"Combined source image). Both images are concatenated and the brought into a\n","#@markdown quadratic format. After that, the results of the type prediction are shown in order of probability. You will get the CN Type ID, the respective probability of the prediction, a link to that type and \n","#@markdown a preview of that type in comparison to your image. Probabilities are given in decimal numbers. The higher the number, the more confident the model is. Despite a high probability, such a result can also be wrong. \n","#@markdown Values like \"1.0\" are the result of rounding processes during the execution of the prediction. Very small probabilities can occur as well, for example \"5.965622e-10\". This expression represents a   \n","#@markdown number where you have to shift the decimal point ten positions to the left. If you click the \"Predict Mint\" button, you will get up to five results and their links for a coin image like in the type \n","#@markdown prediction. Because our mints have a lot of associated types there is no preview image for them.\n","\n","#@markdown It should be noted that only 134 out of 19,000 types are currently trained in this model due to lack of training images. The mint prediction, on the other hand, includes 88 of 92 available mints of\n","#@markdown the CN dataset. The predicted mint results should be more accurate for most coins.\n","# If you want to provide us with feedback to this Colab notebook pls do something....\n","\n","uploader_obv = widgets.FileUpload(\n","    description=\"Select Obverse\",\n","    accept='',  # Accepted file extension e.g. '.txt', '.pdf', 'image/*', 'image/*,.pdf'\n","    multiple=True  # True to accept multiple files upload else False\n",")\n","\n","uploader_rev = widgets.FileUpload(\n","    description=\"Select Reverse\",\n","    accept='',  # Accepted file extension e.g. '.txt', '.pdf', 'image/*', 'image/*,.pdf'\n","    multiple=True  # True to accept multiple files upload else False\n",")\n","\n","ups = widgets.HBox([uploader_obv, uploader_rev])\n","button = widgets.Button(description=\"Upload\")\n","buttond = widgets.Button(description=\"Delete uploaded files\")\n","output = widgets.Output()\n","\n","\n","drop_obv = widgets.Dropdown(\n","    options=os.listdir(obv_storage),\n","    disabled=False, continuous_update=True\n",")\n","drop_rev = widgets.Dropdown(\n","    options=os.listdir(rev_storage),\n","    disabled=False, continuous_update=True\n",")\n","\n","drops = widgets.HBox([drop_obv, drop_rev])\n","buttoncnn_type = widgets.Button(description=\"Predict Type\")\n","buttoncnn_mint = widgets.Button(description=\"Predict Mint\")\n","buttonds = widgets.Button(description=\"Delete Session\")\n","outputcnn = widgets.Output()\n","\n","\n","def predict_on_drop_type(b):\n","    predict_type(obv_storage+drop_obv.value, rev_storage+drop_rev.value)\n","\n","def predict_on_drop_mint(b):\n","    predict_mint(obv_storage+drop_obv.value, rev_storage+drop_rev.value)\n","\n","def on_button_clicked(b):\n","\n","  for img in uploader_obv.value:\n","    uploaded_obv = uploader_obv.value[img]\n","    \n","    with open(obv_storage+img, \"wb\") as fp:\n","      fp.write(uploaded_obv['content'])\n","\n","  for img in uploader_rev.value:\n","    uploaded_rev = uploader_rev.value[img]\n","    \n","    with open(rev_storage+img, \"wb\") as fp:\n","      fp.write(uploaded_rev['content'])\n","\n","  if \".ipynb_checkpoints\" in os.listdir(obv_storage):\n","    os.rmdir(obv_storage+\".ipynb_checkpoints\")\n","\n","  if \".ipynb_checkpoints\" in os.listdir(rev_storage):\n","    os.rmdir(rev_storage+\".ipynb_checkpoints\")\n","\n","  drop_obv.options = os.listdir(obv_storage)\n","  drop_rev.options = os.listdir(rev_storage)\n","  uploader_obv.value.clear()\n","  uploader_obv._counter=0\n","  uploader_rev.value.clear()\n","  uploader_rev._counter=0\n","\n","def delete_temp(b):\n","  for file in os.listdir(obv_storage):\n","    try:\n","      os.remove(obv_storage+file)\n","    except:\n","      pass\n","  for file in os.listdir(rev_storage):\n","    try:\n","      os.remove(rev_storage+file)\n","    except:\n","      pass\n","\n","def delete_sess(b):\n","  clear_output()\n","\n","\n","button.on_click(on_button_clicked)\n","buttond.on_click(delete_temp)\n","buttonds.on_click(delete_sess)\n","display(ups)\n","display(button, output)\n","display(buttond)\n","display(buttonds)\n","display(drops)\n","display(buttoncnn_type)\n","display(buttoncnn_mint)\n","buttoncnn_type.on_click(predict_on_drop_type)\n","buttoncnn_mint.on_click(predict_on_drop_mint)\n","\n","\n","\n"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"IR-for-types-and-mints.ipynb","provenance":[],"authorship_tag":"ABX9TyNUkcQaklOR6yHDEKgJKWHq"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"1f38db7575b44f349ee1fde7f1559227":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_2a2795f71f644e15998da5ab4a98a241","IPY_MODEL_1ed8bc48bc1641cf8c209719ff3fddbf"],"layout":"IPY_MODEL_2cf69a3bbb1142fe91a1397aec0f0fa4"}},"2a2795f71f644e15998da5ab4a98a241":{"model_module":"@jupyter-widgets/controls","model_name":"FileUploadModel","model_module_version":"1.5.0","state":{"_counter":0,"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FileUploadModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"FileUploadView","accept":"","button_style":"","data":[],"description":"Select Obverse","description_tooltip":null,"disabled":false,"error":"","icon":"upload","layout":"IPY_MODEL_b3dfcd3dd1d14a07a3a79d44a1765716","metadata":[],"multiple":true,"style":"IPY_MODEL_694db753a0924048be9f9371071c350c"}},"1ed8bc48bc1641cf8c209719ff3fddbf":{"model_module":"@jupyter-widgets/controls","model_name":"FileUploadModel","model_module_version":"1.5.0","state":{"_counter":0,"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FileUploadModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"FileUploadView","accept":"","button_style":"","data":[],"description":"Select Reverse","description_tooltip":null,"disabled":false,"error":"","icon":"upload","layout":"IPY_MODEL_580f9dc8e5554fe5af329c4940d9268b","metadata":[],"multiple":true,"style":"IPY_MODEL_8aed0295d0b245c7aeb72345db4c5e75"}},"2cf69a3bbb1142fe91a1397aec0f0fa4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b3dfcd3dd1d14a07a3a79d44a1765716":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"694db753a0924048be9f9371071c350c":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"580f9dc8e5554fe5af329c4940d9268b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8aed0295d0b245c7aeb72345db4c5e75":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"03468fcfe183407085a7de2dc5f12a32":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Upload","disabled":false,"icon":"","layout":"IPY_MODEL_c07e049111184fb4a4514d4d60cad3e1","style":"IPY_MODEL_58fb6de8a69a4842b4b178abb9deadae","tooltip":""}},"c07e049111184fb4a4514d4d60cad3e1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58fb6de8a69a4842b4b178abb9deadae":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"4566812b86bf41cbb3f7a9807d8dca68":{"model_module":"@jupyter-widgets/output","model_name":"OutputModel","model_module_version":"1.0.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/output","_model_module_version":"1.0.0","_model_name":"OutputModel","_view_count":null,"_view_module":"@jupyter-widgets/output","_view_module_version":"1.0.0","_view_name":"OutputView","layout":"IPY_MODEL_ca6c927cdec34004aaf3a3f097f015a8","msg_id":"","outputs":[]}},"ca6c927cdec34004aaf3a3f097f015a8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"82d4c88f3eee4f6d9976c25da1e35653":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Delete uploaded files","disabled":false,"icon":"","layout":"IPY_MODEL_bc3ac467b13d47a88e43677b96b3fc2b","style":"IPY_MODEL_220540c641d247eca2a0e8b97a33bff6","tooltip":""}},"bc3ac467b13d47a88e43677b96b3fc2b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"220540c641d247eca2a0e8b97a33bff6":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"959f88fee8734f72a4ab65fcc95bd9c1":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Delete Session","disabled":false,"icon":"","layout":"IPY_MODEL_3b8ee371af324de1bf4850432e5a5b93","style":"IPY_MODEL_07903b379bd945af8d0023d7b6bc17c7","tooltip":""}},"3b8ee371af324de1bf4850432e5a5b93":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"07903b379bd945af8d0023d7b6bc17c7":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"5141989e342645bda0c8cd23c547d3a6":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_828c51f47b3f4009a2ad9cf2d621e78b","IPY_MODEL_0ead38a6197b415db9f33b0cbc00de04"],"layout":"IPY_MODEL_5eb2ce08e756467ca1a2cf2ae1008148"}},"828c51f47b3f4009a2ad9cf2d621e78b":{"model_module":"@jupyter-widgets/controls","model_name":"DropdownModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DropdownModel","_options_labels":[],"_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"DropdownView","description":"","description_tooltip":null,"disabled":false,"index":null,"layout":"IPY_MODEL_13767043fb7f4405b45bd29af9bb55df","style":"IPY_MODEL_8d175bd7abc947c9a62ee7640b12c9af"}},"0ead38a6197b415db9f33b0cbc00de04":{"model_module":"@jupyter-widgets/controls","model_name":"DropdownModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DropdownModel","_options_labels":[],"_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"DropdownView","description":"","description_tooltip":null,"disabled":false,"index":null,"layout":"IPY_MODEL_dcae0b73241b4154a0853a0737b463ec","style":"IPY_MODEL_b42eb490a3414f0e9a904eb43202e3e7"}},"5eb2ce08e756467ca1a2cf2ae1008148":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13767043fb7f4405b45bd29af9bb55df":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d175bd7abc947c9a62ee7640b12c9af":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dcae0b73241b4154a0853a0737b463ec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b42eb490a3414f0e9a904eb43202e3e7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"fe5e8c778a51489392d628cb9ae92660":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Predict Type","disabled":false,"icon":"","layout":"IPY_MODEL_5046131e4c7048e7a9facc8503d05f5e","style":"IPY_MODEL_f0e9eb26075a43518a3d27fd09ebd09e","tooltip":""}},"5046131e4c7048e7a9facc8503d05f5e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f0e9eb26075a43518a3d27fd09ebd09e":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}},"07ab33424f8b4343b3974ed876d5d2a4":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ButtonView","button_style":"","description":"Predict Mint","disabled":false,"icon":"","layout":"IPY_MODEL_ecb1f37055614b86ac5ea85145fb3af7","style":"IPY_MODEL_dd12ba33be9046b484f250f26b54a5f0","tooltip":""}},"ecb1f37055614b86ac5ea85145fb3af7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dd12ba33be9046b484f250f26b54a5f0":{"model_module":"@jupyter-widgets/controls","model_name":"ButtonStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ButtonStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","button_color":null,"font_weight":""}}}}},"nbformat":4,"nbformat_minor":0}